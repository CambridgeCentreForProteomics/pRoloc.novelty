<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Machine learning techniques available in pRoloc • pRoloc</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../jquery.sticky-kit.min.js"></script><script src="../pkgdown.js"></script><meta property="og:title" content="Machine learning techniques available in pRoloc">
<meta property="og:description" content="">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">pRoloc</a>
        <span class="label label-default" data-toggle="tooltip" data-placement="bottom" title="Released package">1.21.2</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../index.html">
    <span class="fa fa-home fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../articles/pRoloc-bayesian.html">Bayesian spatial proteomics with pRoloc</a>
    </li>
    <li>
      <a href="../articles/pRoloc-goannotations.html">Annotating spatial proteomics data</a>
    </li>
    <li>
      <a href="../articles/pRoloc-ml.html">Machine learning techniques available in pRoloc</a>
    </li>
    <li>
      <a href="../articles/pRoloc-transfer-learning.html">A transfer learning algorithm for spatial proteomics</a>
    </li>
    <li>
      <a href="../articles/pRoloc-tutorial.html">Using pRoloc for spatial proteomics data analysis</a>
    </li>
  </ul>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/lgatto/pRoloc">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Machine learning techniques available in pRoloc</h1>
                        <h4 class="author">Laurent Gatto</h4>
                        
      
      <small class="dont-index">Source: <a href="https://github.com/lgatto/pRoloc/blob/master/vignettes/pRoloc-ml.Rmd"><code>vignettes/pRoloc-ml.Rmd</code></a></small>
      <div class="hidden name"><code>pRoloc-ml.Rmd</code></div>

    </div>

    
        <div class="abstract">
      <p class="abstract">Abstract</p>
      <p>This vignette provides a general background about machine learning (ML) methods and concepts, and their application to the analysis of spatial proteomics data in the <em>pRoloc</em> package. See the <code>pRoloc-tutorial</code> vignette for details about the package itself.</p>
    </div>
    
<script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
  document.querySelector("h1").className = "title";
});
</script><script type="text/javascript">
document.addEventListener("DOMContentLoaded", function() {
  var links = document.links;  
  for (var i = 0, linksLength = links.length; i < linksLength; i++)
    if (links[i].hostname != window.location.hostname)
      links[i].target = '_blank';
});
</script><div id="sec:intro" class="section level1">
<h1 class="hasAnchor">
<a href="#sec:intro" class="anchor"></a>Introduction</h1>
<p>For a general practical introduction to <em><a href="http://bioconductor.org/packages/pRoloc">pRoloc</a></em>, readers are referred to the tutorial, available using <code><a href="../articles/pRoloc-tutorial.html">vignette("pRoloc-tutorial", package = "pRoloc")</a></code>. The following document provides a overview of the algorithms available in the package. The respective section describe unsupervised machine learning (USML), supervised machine learning (SML), semi-supervised machine learning (SSML) as implemented in the novelty detection algorithm and transfer learning.</p>
</div>
<div id="data-sets" class="section level1">
<h1 class="hasAnchor">
<a href="#data-sets" class="anchor"></a>Data sets</h1>
<p>We provide 75 test data sets in the <em><a href="http://bioconductor.org/packages/pRolocdata">pRolocdata</a></em> package that can be readily used with <em><a href="http://bioconductor.org/packages/pRoloc">pRoloc</a></em>. The data set can be listed with <em>pRolocdata</em> and loaded with the <em>data</em> function. Each data set, including its origin, is individually documented.</p>
<p>The data sets are distributed as <em>MSnSet</em> instances. Briefly, these are dedicated containers for quantitation data as well as feature and sample meta-data. More details about <em>MSnSet</em>s are available in the <em><a href="http://bioconductor.org/packages/pRoloc">pRoloc</a></em> tutorial and in the <em><a href="http://bioconductor.org/packages/MSnbase">MSnbase</a></em> package, that defined the class.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">"pRolocdata"</span>)
<span class="kw">data</span>(tan2009r1)
tan2009r1</code></pre></div>
<pre><code>## MSnSet (storageMode: lockedEnvironment)
## assayData: 888 features, 4 samples 
##   element names: exprs 
## protocolData: none
## phenoData
##   sampleNames: X114 X115 X116 X117
##   varLabels: Fractions
##   varMetadata: labelDescription
## featureData
##   featureNames: P20353 P53501 ... P07909 (888 total)
##   fvarLabels: FBgn Protein.ID ... markers.tl (16 total)
##   fvarMetadata: labelDescription
## experimentData: use 'experimentData(object)'
##   pubMedIds: 19317464 
## Annotation:  
## - - - Processing information - - -
## Added markers from  'mrk' marker vector. Thu Jul 16 22:53:44 2015 
##  MSnbase version: 1.17.12</code></pre>
<div id="other-omics-data" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#other-omics-data" class="anchor"></a>Other omics data</h2>
<p>While our primary biological domain is quantitative proteomics, with special emphasis on spatial proteomics, the underlying class infrastructure on which <em><a href="http://bioconductor.org/packages/pRoloc">pRoloc</a></em> and implemented in the Bioconductor <em><a href="http://bioconductor.org/packages/MSnbase">MSnbase</a></em> package enables the conversion from/to transcriptomics data, in particular microarray data available as <em>ExpressionSet</em> objects using the <em>as</em> coercion methods (see the <em>MSnSet</em> section in the <code>MSnbase-development</code> vignette). As a result, it is straightforward to apply the methods summarised here in detailed in the other <em><a href="http://bioconductor.org/packages/pRoloc">pRoloc</a></em> vignettes to these other data structures.</p>
</div>
</div>
<div id="sec:usml" class="section level1">
<h1 class="hasAnchor">
<a href="#sec:usml" class="anchor"></a>Unsupervised machine learning</h1>
<p>Unsupervised machine learning refers to clustering, i.e. finding structure in a quantitative, generally multi-dimensional data set of unlabelled data.</p>
<p>Currently, unsupervised clustering facilities are available through the <em>plot2D</em> function and the <em><a href="http://bioconductor.org/packages/MLInterfaces">MLInterfaces</a></em> package <span class="citation">(Carey et al., n.d.)</span>. The former takes an <em>MSnSet</em> instance and represents the data on a scatter plot along the first two principal components. Arbitrary feature meta-data can be represented using different colours and point characters. The reader is referred to the manual page available through <em>?plot2D</em> for more details and examples.</p>
<p><em><a href="http://bioconductor.org/packages/pRoloc">pRoloc</a></em> also implements a <em>MLean</em> method for <em>MSnSet</em> instances, allowing to use the relevant infrastructure with the organelle proteomics framework. Although provides a common interface to unsupervised and numerous supervised algorithms, we refer to the <em><a href="http://bioconductor.org/packages/pRoloc">pRoloc</a></em> tutorial for its usage to several clustering algorithms.</p>
<p><strong>Note</strong> Current development efforts in terms of clustering are described on the <em>Clustering infrastructure</em> wiki page (<a href="https://github.com/lgatto/pRoloc/wiki/Clustering-infrastructure" class="uri">https://github.com/lgatto/pRoloc/wiki/Clustering-infrastructure</a>) and will be incorporated in future version of the package.</p>
</div>
<div id="sec:sml" class="section level1">
<h1 class="hasAnchor">
<a href="#sec:sml" class="anchor"></a>Supervised machine learning</h1>
<p>Supervised machine learning refers to a broad family of classification algorithms. The algorithms learns from a modest set of labelled data points called the training data. Each training data example consists of a pair of inputs: the actual data, generally represented as a vector of numbers and a class label, representing the membership to exactly 1 of multiple possible classes. When there are only two possible classes, on refers to binary classification. The training data is used to construct a model that can be used to classifier new, unlabelled examples. The model takes the numeric vectors of the unlabelled data points and return, for each of these inputs, the corresponding mapped class.</p>
<div id="sec:algo" class="section level2">
<h2 class="hasAnchor">
<a href="#sec:algo" class="anchor"></a>Algorithms used</h2>
<p><strong>k-nearest neighbour (KNN)</strong> Function <em>knn</em> from package <em><a href="http://bioconductor.org/packages/class">class</a></em>. For each row of the test set, the <em>k</em> nearest (in Euclidean distance) training set vectors are found, and the classification is decided by majority vote over the <em>k</em> classes, with ties broken at random. This is a simple algorithm that is often used as baseline classifier. If there are ties for the <em>k</em>th nearest vector, all candidates are included in the vote.</p>
<p><strong>Partial least square DA (PLS-DA)</strong> Function <em>plsda</em> from package . Partial least square discriminant analysis is used to fit a standard PLS model for classification.</p>
<p><strong>Support vector machine (SVM)</strong> A support vector machine constructs a hyperplane (or set of hyperplanes for multiple-class problem), which are then used for classification. The best separation is defined as the hyperplane that has the largest distance (the margin) to the nearest data points in any class, which also reduces the classification generalisation error. To assure liner separation of the classes, the data is transformed using a <em>kernel function</em> into a high-dimensional space, permitting liner separation of the classes.</p>
<p><em><a href="http://bioconductor.org/packages/pRoloc">pRoloc</a></em> makes use of the functions <em>svm</em> from package  and <em>ksvm</em> from .</p>
<p><strong>Artificial neural network (ANN)</strong> Function <em>nnet</em> from package . Fits a single-hidden-layer neural network, possibly with skip-layer connections.</p>
<p><strong>Naive Bayes (NB)</strong> Function <em>naiveBayes</em> from package . Naive Bayes classifier that computes the conditional a-posterior probabilities of a categorical class variable given independent predictor variables using the Bayes rule. Assumes independence of the predictor variables, and Gaussian distribution (given the target class) of metric predictors.</p>
<p><strong>Random Forest (RF)</strong> Function <em>randomForest</em> from package .</p>
<p><strong>Chi-square (<span class="math inline">\(\chi^2\)</span>)</strong> Assignment based on squared differences between a labelled marker and a new feature to be classified. Canonical protein correlation profile method (PCP) uses squared differences between a labelled marker and new features. In <span class="citation">(Andersen et al. 2003)</span>, <span class="math inline">\(\chi^2\)</span> is defined as , i.e. <span class="math inline">\(\chi^{2} = \frac{\sum_{i=1}^{n} (x_i - m_i)^{2}}{n}\)</span>, whereas <span class="citation">(Wiese et al. 2007)</span> divide by the value the squared value by the value of the reference feature in each fraction, i.e. <span class="math inline">\(\chi^{2} = \sum_{i=1}^{n}\frac{(x_i - m_i)^{2}}{m_i}\)</span>, where <span class="math inline">\(x_i\)</span> is normalised intensity of feature <em>x</em> in fraction <em>i</em>, <span class="math inline">\(m_i\)</span> is the normalised intensity of marker <em>m</em> in fraction <em>i</em> and <em>n</em> is the number of fractions available. We will use the former definition.</p>
<p><strong>PerTurbo </strong> From <span class="citation">(Courty, Burger, and Laurent 2011)</span>: PerTurbo, an original, non-parametric and efficient classification method is presented here. In our framework, the manifold of each class is characterised by its Laplace-Beltrami operator, which is evaluated with classical methods involving the graph Laplacian. The classification criterion is established thanks to a measure of the magnitude of the spectrum perturbation of this operator. The first experiments show good performances against classical algorithms of the state-of-the-art. Moreover, from this measure is derived an efficient policy to design sampling queries in a context of active learning. Performances collected over toy examples and real world datasets assess the qualities of this strategy.</p>
<p>The PerTurbo implementation comes from the <em><a href="http://bioconductor.org/packages/pRoloc">pRoloc</a></em> packages.</p>
</div>
<div id="estimating-algorithm-parameters" class="section level2">
<h2 class="hasAnchor">
<a href="#estimating-algorithm-parameters" class="anchor"></a>Estimating algorithm parameters</h2>
<p>It is essential when applying any of the above classification algorithms, to wisely set the algorithm parameters, as these can have an important effect on the classification. Such parameters are for example the width <em>sigma</em> of the Radial Basis Function (Gaussian kernel) <span class="math inline">\(exp(-\sigma \| x - x' \|^2 )\)</span> and the <em>cost</em> (slack) parameter (controlling the tolerance to mis-classification) of our SVM classifier. The number of neighbours <em>k</em> of the KNN classifier is equally important as will be discussed in this sections.</p>
<p>The <a href="#fig:knnboundaries">next figure</a> illustrates the effect of different choices of <span class="math inline">\(k\)</span> using organelle proteomics data from <span class="citation">(T. P. J. Dunkley et al. 2006)</span> (<em>dunkley2006</em> from <em><a href="http://bioconductor.org/packages/pRolocdata">pRolocdata</a></em>). As highlighted in the squared region, we can see that using a low <span class="math inline">\(k\)</span> (<em>k = 1</em> on the left) will result in very specific classification boundaries that precisely follow the contour or our marker set as opposed to a higher number of neighbours (<em>k = 8</em> on the right). While one could be tempted to believe that <em>optimised</em> classification boundaries are preferable, it is essential to remember that these boundaries are specific to the marker set used to construct them, while there is absolutely no reason to expect these regions to faithfully separate any new data points, in particular proteins that we wish to classify to an organelle. In other words, the highly specific <em>k = 1</em> classification boundaries are <em>over-fitted</em> for the marker set or, in other words, lack generalisation to new instances. We will demonstrate this using simulated data taken from <span class="citation">(James et al. 2013)</span> and show what detrimental effect <em>over-fitting</em> has on new data.</p>
<div class="figure">
<img src="Figures/knnboundaries.png" alt="Classification boundaries using k=1 or k=8 on the dunkley2006 data." id="fig:knnboundaries"><p class="caption">Classification boundaries using <span class="math inline">\(k=1\)</span> or <span class="math inline">\(k=8\)</span> on the <code>dunkley2006</code> data.</p>
</div>
<p>The <a href="#fig:ISL1">figure below</a> uses 2 <em>x</em> 100 simulated data points belonging to either of the orange or blue classes. The genuine classes for all the points is known (solid lines) and the KNN algorithm has been applied using <em>k = 1</em> (left) and <em>k = 100</em> (right) respectively (purple dashed lines). As in our organelle proteomics examples, we observe that when k = 1, the decision boundaries are overly flexible and identify patterns in the data that do not reflect to correct boundaries (in such cases, the classifier is said to have low bias but very high variance). When a large <em>k</em> is used, the classifier becomes inflexible and produces approximate and nearly linear separation boundaries (such a classifier is said to have low variance but high bias). On this simulated data set, neither <em>k = 1</em> nor <em>k = 100</em> give good predictions and have test error rates (i.e. the proportion of wrongly classified points) of 0.1695 and 0.1925, respectively.</p>
<div class="figure">
<img src="Figures/ISL-2_16.png" alt="The KNN classifier using k = 1 (left, solid classification boundaries) and k = 100 (right, solid classification boundaries) compared the Bayes decision boundaries (see original material for details). Reproduced with permission from (James et al. 2013)." id="fig:ISL1"><p class="caption">The KNN classifier using <span class="math inline">\(k = 1\)</span> (left, solid classification boundaries) and <span class="math inline">\(k = 100\)</span> (right, solid classification boundaries) compared the Bayes decision boundaries (see original material for details). Reproduced with permission from <span class="citation">(James et al. 2013)</span>.</p>
</div>
<p>To quantify the effect of flexibility and lack thereof in defining the classification boundaries, <span class="citation">(James et al. 2013)</span> calculate the classification error rates using training data (training error rate) and testing data (testing error rate). The latter is completely new data that was not used to assess the model error rate when defining algorithm parameters; one often says that the model used for classification has not <em>seen</em> this data. If the model performs well on new data, it is said to generalise well. This is a quality that is required in most cases, and in particular in our organelle proteomics experiments where the training data corresponds to our marker sets. Figure @ref{fig:ISL2} plots the respective training and testing error rates as a function of <em>1/k</em> which is a reflection of the flexibility/complexity of our model; when <em>1/k = 1</em>, i.e. <em>k = 1</em> (far right), we have a very flexible model with the risk of over-fitting. Greater values of <em>k</em> (towards the left) characterise less flexible models. As can be seen, high values of <em>k</em> produce poor performance for both training and testing data. However, while the training error steadily decreases when the model complexity increases (smaller <em>k</em>), the testing error rate displays a typical U-shape: at a value around <em>k = 10</em>, the testing error rate reaches a minimum and then starts to increase due to over-fitting to the training data and lack of generalisation of the model.</p>
<div class="figure">
<img src="Figures/ISL-2_17.png" alt="Effect of train and test error with respect to model complexity. The former decreases for lower values of k while the test error reaches a minimum around k = 10 before increasing again. Reproduced with permission from (James et al. 2013)." id="fig:ISL2"><p class="caption">Effect of train and test error with respect to model complexity. The former decreases for lower values of <span class="math inline">\(k\)</span> while the test error reaches a minimum around <span class="math inline">\(k = 10\)</span> before increasing again. Reproduced with permission from <span class="citation">(James et al. 2013)</span>.</p>
</div>
<p>These results show that adequate optimisation of the model parameters are essential to avoid either too flexible models (that do not generalise well to new data) or models that do not describe the decision boundaries adequately. Such parameter selection is achieved by cross validation, where the initial marker proteins are separated into training data used to build classification models and independent testing data used to assess the model on new data.</p>
<p>We recommend the book <em>An Introduction to Statistical Learning</em> (<a href="http://www-bcf.usc.edu/~gareth/ISL/" class="uri">http://www-bcf.usc.edu/~gareth/ISL/</a>) by <span class="citation">(James et al. 2013)</span> for a more detailed introduction of machine learning.</p>
</div>
<div id="default-analysis-scheme" class="section level2">
<h2 class="hasAnchor">
<a href="#default-analysis-scheme" class="anchor"></a>Default analysis scheme</h2>
<p>Below, we present a typical classification analysis using <em><a href="http://bioconductor.org/packages/pRoloc">pRoloc</a></em>. The analysis typically consists of two steps. The first one is to optimise the classifier parameters to be used for training and testing (see above). A range of parameters are tested using the labelled data, for which the labels are known. For each set of parameters, we hide the labels of a subset of labelled data and use the other part to train a model and apply in on the testing data with hidden labels. The comparison of the estimated and expected labels enables to assess the validity of the model and hence the adequacy if the parameters. Once adequate parameters have been identified, they are used to infer a model on the complete organelle marker set and used to infer the sub-cellular location of the unlabelled examples.</p>
</div>
<div id="parameter-optimisation" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#parameter-optimisation" class="anchor"></a>Parameter optimisation</h2>
<p>Algorithmic performance is estimated using a stratified 20/80 partitioning. The 80% partitions are subjected to 5-fold cross-validation in order to optimise free parameters via a grid search, and these parameters are then applied to the remaining 20%. The procedure is repeated <em>n = 100</em> <code>times</code> to sample <em>n</em> accuracy metrics (see below) values using <em>n</em>, possibly different, optimised parameters for evaluation.</p>
<p>Models accuracy is evaluated using the F1 score, <span class="math inline">\(F1 = 2 ~ \frac{precision \times recall}{precision + recall}\)</span>, calculated as the harmonic mean of the precision (<span class="math inline">\(precision = \frac{tp}{tp+fp}\)</span>, a measure of <em>exactness</em> – returned output is a relevant result) and recall (<span class="math inline">\(recall=\frac{tp}{tp+fn}\)</span>, a measure of <em>completeness</em> – indicating how much was missed from the output). What we are aiming for are high generalisation accuracy, i.e high <span class="math inline">\(F1\)</span>, indicating that the marker proteins in the test data set are consistently correctly assigned by the algorithms.</p>
<p>The results of the optimisation procedure are stored in an <em>GenRegRes</em> object that can be inspected, plotted and best parameter pairs can be extracted.</p>
<p>For a given algorithm <code>alg</code>, the corresponding parameter optimisation function is names <em>algOptimisation</em> or, equivalently, <em>algOptimization</em>. See the table below for details. A description of each of the respective model parameters is provided in the optimisation function manuals, available through <em>?algOptimisation</em>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">params &lt;-<span class="st"> </span><span class="kw"><a href="../reference/svmOptimisation.html">svmOptimisation</a></span>(tan2009r1, <span class="dt">times =</span> <span class="dv">10</span>,
                          <span class="dt">xval =</span> <span class="dv">5</span>, <span class="dt">verbose =</span> <span class="ot">FALSE</span>)
params</code></pre></div>
<pre><code>## Object of class "GenRegRes"
## Algorithm: svm 
## Hyper-parameters:
##  cost: 0.0625 0.125 0.25 0.5 1 2 4 8 16
##  sigma: 0.001 0.01 0.1 1 10 100
## Design:
##  Replication: 10 x 5-fold X-validation
##  Partitioning: 0.2/0.8 (test/train)
## Results
##  macro F1:
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.7253  0.7684  0.8486  0.8294  0.8673  0.9546 
##  best sigma: 1 0.1   
##  best cost: 16 4 8   
## Use getWarnings() to see warnings.</code></pre>
</div>
<div id="classification" class="section level2 unnumbered">
<h2 class="hasAnchor">
<a href="#classification" class="anchor"></a>Classification</h2>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tan2009r1 &lt;-<span class="st"> </span><span class="kw"><a href="../reference/svmClassification.html">svmClassification</a></span>(tan2009r1, params)
tan2009r1</code></pre></div>
<pre><code>## MSnSet (storageMode: lockedEnvironment)
## assayData: 888 features, 4 samples 
##   element names: exprs 
## protocolData: none
## phenoData
##   sampleNames: X114 X115 X116 X117
##   varLabels: Fractions
##   varMetadata: labelDescription
## featureData
##   featureNames: P20353 P53501 ... P07909 (888 total)
##   fvarLabels: FBgn Protein.ID ... svm.scores (18 total)
##   fvarMetadata: labelDescription
## experimentData: use 'experimentData(object)'
##   pubMedIds: 19317464 
## Annotation:  
## - - - Processing information - - -
## Added markers from  'mrk' marker vector. Thu Jul 16 22:53:44 2015 
## Performed svm prediction (sigma=1 cost=8) Fri Jun  1 17:07:24 2018 
##  MSnbase version: 1.17.12</code></pre>
</div>
<div id="customising-model-parameters" class="section level2">
<h2 class="hasAnchor">
<a href="#customising-model-parameters" class="anchor"></a>Customising model parameters</h2>
<p>Below we illustrate how to weight different classes according to the number of labelled instances, where large sets are down weighted. This strategy can help with imbalanced designs.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">w &lt;-<span class="st"> </span><span class="kw">table</span>(<span class="kw">fData</span>(<span class="kw"><a href="../reference/markerMSnSet.html">markerMSnSet</a></span>(dunkley2006))<span class="op">$</span>markers)
wpar &lt;-<span class="st"> </span><span class="kw"><a href="../reference/svmOptimisation.html">svmOptimisation</a></span>(dunkley2006, <span class="dt">class.weights =</span> w)
wres &lt;-<span class="st"> </span><span class="kw"><a href="../reference/svmClassification.html">svmClassification</a></span>(dunkley2006, wpar, <span class="dt">class.weights =</span> w)</code></pre></div>
<table class="table">
<thead><tr class="header">
<th align="left">parameter optimisation</th>
<th align="left">classification</th>
<th align="left">algorithm</th>
<th align="left">package</th>
</tr></thead>
<tbody>
<tr class="odd">
<td align="left">knnOptimisation</td>
<td align="left">knnClassification</td>
<td align="left">nearest neighbour</td>
<td align="left">class</td>
</tr>
<tr class="even">
<td align="left">knntlOptimisation</td>
<td align="left">knntlClassification</td>
<td align="left">nearest neighbour transfer learning</td>
<td align="left">pRoloc</td>
</tr>
<tr class="odd">
<td align="left">ksvmOptimisation</td>
<td align="left">ksvmClassification</td>
<td align="left">support vector machine</td>
<td align="left">kernlab</td>
</tr>
<tr class="even">
<td align="left">nbOptimisation</td>
<td align="left">nbClassification</td>
<td align="left">naive bayes</td>
<td align="left">e1071</td>
</tr>
<tr class="odd">
<td align="left">nnetOptimisation</td>
<td align="left">nnetClassification</td>
<td align="left">neural networks</td>
<td align="left">nnet</td>
</tr>
<tr class="even">
<td align="left">perTurboOptimisation</td>
<td align="left">perTurboClassification</td>
<td align="left">PerTurbo</td>
<td align="left">pRoloc</td>
</tr>
<tr class="odd">
<td align="left">plsdaOptimisation</td>
<td align="left">plsdaClassification</td>
<td align="left">partial least square</td>
<td align="left">caret</td>
</tr>
<tr class="even">
<td align="left">rfOptimisation</td>
<td align="left">rfClassification</td>
<td align="left">random forest</td>
<td align="left">randomForest</td>
</tr>
<tr class="odd">
<td align="left">svmOptimisation</td>
<td align="left">svmClassification</td>
<td align="left">support vector machine</td>
<td align="left">e1071</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="comparison-of-different-classifiers" class="section level1">
<h1 class="hasAnchor">
<a href="#comparison-of-different-classifiers" class="anchor"></a>Comparison of different classifiers</h1>
<p>Several supervised machine learning algorithms have already been applied to organelle proteomics data classification: partial least square discriminant analysis in <span class="citation">(T. P. J. Dunkley et al. 2006, Tan2009)</span>, support vector machines (SVMs) in <span class="citation">(M. W. B. Trotter et al. 2010)</span>, random forest in <span class="citation">(Ohta et al. 2010)</span>, neural networks in <span class="citation">(Tardif et al. 2012)</span>, naive Bayes <span class="citation">(Nikolovski et al. 2012)</span>. In our HUPO 2011 poster<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>, we show that different classification algorithms provide very similar performance. We have extended this comparison on various datasets distributed in the <em><a href="http://bioconductor.org/packages/pRolocdata">pRolocdata</a></em> package. On figure @ref{fig:f1box}, we illustrate how different algorithms reach very similar performances on most of our test datasets.</p>
<div class="figure">
<img src="Figures/F1boxplots.png" alt="Comparison of classification performances of several contemporary classification algorithms on data from the pRolocdata package." id="fig:f1box"><p class="caption">Comparison of classification performances of several contemporary classification algorithms on data from the <em><a href="http://bioconductor.org/packages/pRolocdata">pRolocdata</a></em> package.</p>
</div>
</div>
<div id="sec:bayes" class="section level1">
<h1 class="hasAnchor">
<a href="#sec:bayes" class="anchor"></a>Bayesian generative models</h1>
<p>We also offer generative models that, as opposed to the descriptive classifier presented above, explicitly model the spatial proteomics data. In <code>pRoloc</code>, we probose two models using T-augmented Gaussian mixtures using repectively a Expectration-Maximisation approach to <em>maximum a posteriori</em> estimation of the model parameters (TAGM-MAP), and an MCMC approach (TAGM-MCMC) that enables a proteome-wide uncertainty quantitation. These methods are described in the <em>pRoloc-bayesian</em> vignette.</p>
<p>For a details description of the methods and their validation, please refer to <span class="citation">(Crook et al. 2018)</span>:</p>
<blockquote>
<p>A Bayesian Mixture Modelling Approach For Spatial Proteomics Oliver M Crook, Claire M Mulvey, Paul D. W. Kirk, Kathryn S Lilley, Laurent Gatto bioRxiv 282269; doi: <a href="https://doi.org/10.1101/282269" class="uri">https://doi.org/10.1101/282269</a></p>
</blockquote>
</div>
<div id="sec:ssml" class="section level1">
<h1 class="hasAnchor">
<a href="#sec:ssml" class="anchor"></a>Semi-supervised machine learning</h1>
<p>The <em>phenoDisco</em> algorithm is a semi-supervised novelty detection method by <span class="citation">(Lisa M Breckels et al. 2013)</span> (<a href="#fig:pd">figure below</a>). It uses the labelled (i.e. markers, noted <span class="math inline">\(D_L\)</span>) and unlabelled (i.e. proteins of unknown localisation, noted <span class="math inline">\(D_U\)</span>) sets of the input data. The algorithm is repeated <span class="math inline">\(N\)</span> times (the <code>times</code> argument in the <em>phenoDisco</em> function). At each iteration, each organelle class <span class="math inline">\(D_{L}^{i}\)</span> and the unlabelled complement are clustered using Gaussian mixture modelling. While unlabelled members that systematically cluster with <span class="math inline">\(D_{L}^{i}\)</span> and pass outlier detection are labelled as new putative members of class <span class="math inline">\(i\)</span>, any example of <span class="math inline">\(D_U\)</span> which are not merged with any any of the <span class="math inline">\(D_{L}^{i}\)</span> and are consistently clustered together throughout the <span class="math inline">\(N\)</span> iterations are considered members of a new phenotype.</p>
<div class="figure">
<img src="Figures/phenodisco.png" alt="The PhenoDisco iterative algorithm." id="fig:pd"><p class="caption">The PhenoDisco iterative algorithm.</p>
</div>
</div>
<div id="sec:tl" class="section level1">
<h1 class="hasAnchor">
<a href="#sec:tl" class="anchor"></a>Transfer learning</h1>
<p>When multiple sources of data are available, it is often beneficial to take all or several into account with the aim of increasing the information to tackle a problem of interest. While it is at times possible to combine these different sources of data, this can lead to substantially harm to performance of the analysis when the different data sources are of variable signal-to-noise ratio or the data are drawn from different domains and recorded by different encoding (quantitative and binary, for example). If we defined the following two data source</p>
<ol style="list-style-type: decimal">
<li>
<em>primary</em> data, of high signal-to-noise ratio, but general available in limited amounts;</li>
<li>
<em>auxiliary</em> data, of limited signal-to-noise, and available in large amounts;</li>
</ol>
<p>then, a <em>transfer learning</em> algorithm will efficiently support/complement the primary target domain with auxiliary data features without compromising the integrity of our primary data.</p>
<p>We have developed a transfer learning framework <span class="citation">(L M Breckels et al. 2016)</span> and applied to the analysis of spatial proteomics data, as described in the <code>pRoloc-transfer-learning</code> vignette.</p>
</div>
<div id="session-information" class="section level1">
<h1 class="hasAnchor">
<a href="#session-information" class="anchor"></a>Session information</h1>
<p>All software and respective versions used to produce this document are listed below.</p>
<pre><code>## R Under development (unstable) (2018-04-02 r74505)
## Platform: x86_64-pc-linux-gnu (64-bit)
## Running under: Ubuntu 14.04.5 LTS
## 
## Matrix products: default
## BLAS: /usr/lib/atlas-base/atlas/libblas.so.3.0
## LAPACK: /usr/lib/lapack/liblapack.so.3.0
## 
## locale:
##  [1] LC_CTYPE=en_GB.UTF-8       LC_NUMERIC=C              
##  [3] LC_TIME=en_GB.UTF-8        LC_COLLATE=en_GB.UTF-8    
##  [5] LC_MONETARY=en_GB.UTF-8    LC_MESSAGES=en_GB.UTF-8   
##  [7] LC_PAPER=en_GB.UTF-8       LC_NAME=C                 
##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
## [11] LC_MEASUREMENT=en_GB.UTF-8 LC_IDENTIFICATION=C       
## 
## attached base packages:
## [1] stats4    parallel  stats     graphics  grDevices utils     datasets 
## [8] methods   base     
## 
## other attached packages:
##  [1] pRolocdata_1.19.1    pRoloc_1.21.2        MLInterfaces_1.61.1 
##  [4] cluster_2.0.7-1      annotate_1.59.0      XML_3.98-1.11       
##  [7] AnnotationDbi_1.43.1 IRanges_2.15.13      S4Vectors_0.19.5    
## [10] MSnbase_2.7.1        ProtGenerics_1.13.0  BiocParallel_1.15.5 
## [13] mzR_2.15.1           Rcpp_0.12.17         Biobase_2.41.0      
## [16] BiocGenerics_0.27.0  knitr_1.20           BiocStyle_2.9.2     
## 
## loaded via a namespace (and not attached):
##   [1] tidyselect_0.2.4      RSQLite_2.1.1         htmlwidgets_1.2      
##   [4] grid_3.6.0            trimcluster_0.1-2     lpSolve_5.6.13       
##   [7] rda_1.0.2-2           munsell_0.4.3         codetools_0.2-15     
##  [10] preprocessCore_1.43.0 withr_2.1.2           colorspace_1.3-2     
##  [13] BiocInstaller_1.31.1  highr_0.6             geometry_0.3-6       
##  [16] robustbase_0.93-0     dimRed_0.1.0          mzID_1.19.0          
##  [19] mnormt_1.5-5          hwriter_1.3.2         bit64_0.9-7          
##  [22] ggvis_0.4.3           rprojroot_1.3-2       coda_0.19-1          
##  [25] ipred_0.9-6           xfun_0.1              randomForest_4.6-14  
##  [28] diptest_0.75-7        R6_2.2.2              doParallel_1.0.11    
##  [31] flexmix_2.3-14        DRR_0.0.3             bitops_1.0-6         
##  [34] assertthat_0.2.0      promises_1.0.1        scales_0.5.0         
##  [37] nnet_7.3-12           gtable_0.2.0          affy_1.59.0          
##  [40] ddalpha_1.3.3         timeDate_3043.102     rlang_0.2.0          
##  [43] CVST_0.2-1            genefilter_1.63.0     RcppRoll_0.2.2       
##  [46] splines_3.6.0         lazyeval_0.2.1        ModelMetrics_1.1.0   
##  [49] impute_1.55.0         hexbin_1.27.2         broom_0.4.4          
##  [52] yaml_2.1.19           reshape2_1.4.3        abind_1.4-5          
##  [55] threejs_0.3.1         crosstalk_1.0.0       backports_1.1.2      
##  [58] httpuv_1.4.3          caret_6.0-80          tools_3.6.0          
##  [61] lava_1.6.1            bookdown_0.7          psych_1.8.4          
##  [64] ggplot2_2.2.1         affyio_1.51.0         RColorBrewer_1.1-2   
##  [67] proxy_0.4-22          plyr_1.8.4            base64enc_0.1-3      
##  [70] progress_1.1.2        zlibbioc_1.27.0       purrr_0.2.4          
##  [73] RCurl_1.95-4.10       prettyunits_1.0.2     rpart_4.1-13         
##  [76] viridis_0.5.1         sampling_2.8          sfsmisc_1.1-2        
##  [79] LaplacesDemon_16.1.0  fs_1.2.2              magrittr_1.5         
##  [82] pcaMethods_1.73.0     mvtnorm_1.0-8         whisker_0.3-2        
##  [85] mime_0.5              evaluate_0.10.1       xtable_1.8-2         
##  [88] mclust_5.4            gridExtra_2.3         compiler_3.6.0       
##  [91] biomaRt_2.37.1        tibble_1.4.2          crayon_1.3.4         
##  [94] htmltools_0.3.6       segmented_0.5-3.0     later_0.7.2          
##  [97] tidyr_0.8.1           lubridate_1.7.4       DBI_1.0.0            
## [100] magic_1.5-8           MASS_7.3-50           fpc_2.1-11           
## [103] Matrix_1.2-14         vsn_3.49.0            gdata_2.18.0         
## [106] mlbench_2.1-1         bindr_0.1.1           gower_0.1.2          
## [109] igraph_1.2.1          pkgconfig_2.0.1       pkgdown_1.0.0        
## [112] foreign_0.8-70        recipes_0.1.2         MALDIquant_1.17      
## [115] xml2_1.2.0            roxygen2_6.0.1        foreach_1.4.4        
## [118] prodlim_2018.04.18    stringr_1.3.1         digest_0.6.15        
## [121] pls_2.6-0             rmarkdown_1.9         dendextend_1.8.0     
## [124] kernlab_0.9-26        shiny_1.1.0           gtools_3.5.0         
## [127] commonmark_1.5        modeltools_0.2-21     nlme_3.1-137         
## [130] bindrcpp_0.2.2        desc_1.2.0            viridisLite_0.3.0    
## [133] limma_3.37.1          pillar_1.2.2          lattice_0.20-35      
## [136] httr_1.3.1            DEoptimR_1.0-8        survival_2.42-3.1    
## [139] glue_1.2.0            FNN_1.1               gbm_2.1.3            
## [142] prabclus_2.2-6        iterators_1.0.9       bit_1.1-13           
## [145] class_7.3-14          stringi_1.2.2         mixtools_1.1.0       
## [148] blob_1.1.1            memoise_1.1.0         dplyr_0.7.5          
## [151] e1071_1.6-8</code></pre>
</div>
<div id="references" class="section level1 unnumbered">
<h1 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h1>
<div id="refs" class="references">
<div id="ref-Andersen2003">
<p>Andersen, Jens S., Christopher J. Wilkinson, Thibault Mayor, Peter Mortensen, Erich A. Nigg, and Matthias Mann. 2003. “Proteomic Characterization of the Human Centrosome by Protein Correlation Profiling.” <em>Nature</em> 426 (6966). Center for Experimental BioInformatics, Department of Biochemistry; Molecular Biology, University of Southern Denmark, Campusvej 55, DK-5230 Odense M, Denmark.: 570–74. doi:<a href="https://doi.org/10.1038/nature02166">10.1038/nature02166</a>.</p>
</div>
<div id="ref-Breckels:2016">
<p>Breckels, L M, S B Holden, D Wojnar, C M Mulvey, A Christoforou, A Groen, M W Trotter, O Kohlbacher, K S Lilley, and L Gatto. 2016. “Learning from Heterogeneous Data Sources: An Application in Spatial Proteomics.” <em>PLoS Comput Biol</em> 12 (5): e1004920. doi:<a href="https://doi.org/10.1371/journal.pcbi.1004920">10.1371/journal.pcbi.1004920</a>.</p>
</div>
<div id="ref-Breckels2013">
<p>Breckels, Lisa M, Laurent Gatto, Andy Christoforou, Arnoud J Groen, Kathryn S Lilley, and Matthew W B Trotter. 2013. “The Effect of Organelle Discovery Upon Sub-Cellular Protein Localisation.” <em>J Proteomics</em>, March. doi:<a href="https://doi.org/10.1016/j.jprot.2013.02.019">10.1016/j.jprot.2013.02.019</a>.</p>
</div>
<div id="ref-MLInterfaces">
<p>Carey, Vince, Robert Gentleman, Jess Mar, and contributions from Jason Vertrees, and Laurent Gatto. n.d. <em>MLInterfaces: Uniform Interfaces to R Machine Learning Procedures for Data in Bioconductor Containers</em>.</p>
</div>
<div id="ref-perturbo">
<p>Courty, Nicolas, Thomas Burger, and Johann Laurent. 2011. “PerTurbo: A New Classification Algorithm Based on the Spectrum Perturbations of the Laplace-Beltrami Operator.” In <em>In the Proceedings of Ecml/Pkdd (1)</em>, edited by Dimitrios Gunopulos, Thomas Hofmann, Donato Malerba, and Michalis Vazirgiannis, 6911:359–74. Lecture Notes in Computer Science. Springer.</p>
</div>
<div id="ref-Crook:2018">
<p>Crook, Oliver M, Claire M Mulvey, Paul D. W. Kirk, Kathryn S Lilley, and Laurent Gatto. 2018. “A Bayesian Mixture Modelling Approach for Spatial Proteomics.” <em>bioRxiv</em>. Cold Spring Harbor Laboratory. doi:<a href="https://doi.org/10.1101/282269">10.1101/282269</a>.</p>
</div>
<div id="ref-Dunkley2006">
<p>Dunkley, Tom P. J., Svenja Hester, Ian P. Shadforth, John Runions, Thilo Weimar, Sally L. Hanton, Julian L. Griffin, et al. 2006. “Mapping the Arabidopsis Organelle Proteome.” <em>Proc Natl Acad Sci USA</em> 103 (17). Department of Biochemistry, University of Cambridge, Downing Site, Cambridge CB2 1QW, United Kingdom.: 6518–23. doi:<a href="https://doi.org/10.1073/pnas.0506958103">10.1073/pnas.0506958103</a>.</p>
</div>
<div id="ref-ISLwR">
<p>James, G., D. Witten, T. Hastie, and R. Tibshirani. 2013. <em>An Introduction to Statistical Learning: With Applications in R</em>. Springer Texts in Statistics. Springer.</p>
</div>
<div id="ref-Nikolovski2012">
<p>Nikolovski, N, D Rubtsov, M P Segura, G P Miles, T J Stevens, T P Dunkley, S Munro, K S Lilley, and P Dupree. 2012. “Putative Glycosyltransferases and Other Plant Golgi Apparatus Proteins Are Revealed by LOPIT Proteomics.” <em>Plant Physiol</em> 160 (2): 1037–51. doi:<a href="https://doi.org/10.1104/pp.112.204263">10.1104/pp.112.204263</a>.</p>
</div>
<div id="ref-Ohta2010">
<p>Ohta, S, J C Bukowski-Wills, L Sanchez-Pulido, Fde L Alves, L Wood, Z A Chen, M Platani, et al. 2010. “The Protein Composition of Mitotic Chromosomes Determined Using Multiclassifier Combinatorial Proteomics.” <em>Cell</em> 142 (5): 810–21. doi:<a href="https://doi.org/10.1016/j.cell.2010.07.047">10.1016/j.cell.2010.07.047</a>.</p>
</div>
<div id="ref-Tardif2012">
<p>Tardif, M, A Atteia, M Specht, G Cogne, N Rolland, S Brugière, M Hippler, et al. 2012. “PredAlgo: A New Subcellular Localization Prediction Tool Dedicated to Green Algae.” <em>Mol Biol Evol</em> 29 (12): 3625–39. doi:<a href="https://doi.org/10.1093/molbev/mss178">10.1093/molbev/mss178</a>.</p>
</div>
<div id="ref-Trotter2010">
<p>Trotter, Matthew W. B., Pawel G. Sadowski, Tom P. J. Dunkley, Arnoud J. Groen, and Kathryn S. Lilley. 2010. “Improved Sub-Cellular Resolution via Simultaneous Analysis of Organelle Proteomics Data Across Varied Experimental Conditions.” <em>PROTEOMICS</em> 10 (23). WILEY-VCH Verlag: 4213–9. doi:<a href="https://doi.org/10.1002/pmic.201000359">10.1002/pmic.201000359</a>.</p>
</div>
<div id="ref-Wiese2007">
<p>Wiese, Sebastian, Thomas Gronemeyer, Rob Ofman, Markus Kunze, Cláudia P. Grou, José A. Almeida, Martin Eisenacher, et al. 2007. “Proteomics Characterization of Mouse Kidney Peroxisomes by Tandem Mass Spectrometry and Protein Correlation Profiling.” <em>Mol Cell Proteomics</em> 6 (12). Medizinisches Proteom-Center, Ruhr-Universitaet Bochum, Universitaetsstrasse 150, 44780 Bochum, Germany.: 2045–57. doi:<a href="https://doi.org/10.1074/mcp.M700169-MCP200">10.1074/mcp.M700169-MCP200</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr>
<ol>
<li id="fn1"><p>Gatto, Laurent; Breckels, Lisa M.; Trotter, Matthew W.B.; Lilley, Kathryn S. (2011): <code>pRoloc</code> - A unifying bioinformatics framework for organelle proteomics. <a href="https://doi.org/10.6084/m9.figshare.5042965.v1" class="uri">https://doi.org/10.6084/m9.figshare.5042965.v1</a><a href="#fnref1">↩</a></p></li>
</ol>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#sec:intro">Introduction</a></li>
      <li>
<a href="#data-sets">Data sets</a><ul class="nav nav-pills nav-stacked">
<li><a href="#other-omics-data">Other omics data</a></li>
      </ul>
</li>
      <li><a href="#sec:usml">Unsupervised machine learning</a></li>
      <li>
<a href="#sec:sml">Supervised machine learning</a><ul class="nav nav-pills nav-stacked">
<li><a href="#sec:algo">Algorithms used</a></li>
      <li><a href="#estimating-algorithm-parameters">Estimating algorithm parameters</a></li>
      <li><a href="#default-analysis-scheme">Default analysis scheme</a></li>
      <li><a href="#parameter-optimisation">Parameter optimisation</a></li>
      <li><a href="#classification">Classification</a></li>
      <li><a href="#customising-model-parameters">Customising model parameters</a></li>
      </ul>
</li>
      <li><a href="#comparison-of-different-classifiers">Comparison of different classifiers</a></li>
      <li><a href="#sec:bayes">Bayesian generative models</a></li>
      <li><a href="#sec:ssml">Semi-supervised machine learning</a></li>
      <li><a href="#sec:tl">Transfer learning</a></li>
      <li><a href="#session-information">Session information</a></li>
      <li><a href="#references">References</a></li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Laurent Gatto, Lisa Breckels, Oliver Crook.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  

  </body>
</html>
