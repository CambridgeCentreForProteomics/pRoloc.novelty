%\VignetteEngine{knitr::knitr}
%\VignetteIndexEntry{A transfer learning algorithm for spatial proteomics}
%\VignetteKeywords{Bioinformatics, Machine learning, Organelle, Spatial Proteomics}
%\VignettePackage{pRoloc}

\documentclass[12pt, oneside]{article}

<<style, eval=TRUE, echo=FALSE, results="asis">>=
BiocStyle::latex()
@


\author{
  Lisa M. Breckels and Laurent Gatto\footnote{\email{lg390@cam.ac.uk}}\\
  Computational Proteomics Unit\\
  University of Cambridge, UK
}


\bioctitle[\Biocpkg{pRoloc} \textit{theta}]{A transfer learning
  algorithm for spatial proteomics}

\begin{document}

\maketitle

%% Abstract and keywords %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip 0.3in minus 0.1in
\hrule
\begin{abstract}
  This vignette illustrates the application of a \emph{transfer
    learning} algorithm to assign protein to sub-cellular
  localisations. The \emph{thetaClassification} algorithm will combine
  a \emph{primary} experiment spatial proteomics data (LOPIT, PCP,
  etc.) and an \emph{auxiliary} binary data (for example Gene Ontology
  terms) to improve the sub-cellular assignment given an optimal
  combination of these data sources.
\end{abstract}
\textit{Keywords}: Bioinformatics, organelle, spatial proteomics,
machine learning \vskip 0.1in minus 0.05in \hrule \vskip 0.2in minus
0.1in
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\tableofcontents

<<env, include=FALSE, echo=FALSE, cache=FALSE>>=
library("knitr")
opts_chunk$set(fig.align = 'center', 
               fig.show = 'hold', 
               par = TRUE,
               prompt = TRUE,
               eval = TRUE,
               stop_on_error = 1L,
               comment = NA)
options(replace.assign = TRUE, 
        width = 55)
suppressPackageStartupMessages(library("dplyr"))
suppressPackageStartupMessages(library("MSnbase"))
suppressWarnings(suppressPackageStartupMessages(library("pRoloc")))
suppressPackageStartupMessages(library("pRolocdata"))
suppressPackageStartupMessages(library("class"))
set.seed(1)
@ 
%%$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}\label{sec:intro} 

Our main data source to study protein sub-cellular localisation are
high-throughput mass spectrometry-based experiments such as LOPIT, PCP
and similar designs (see \cite{Gatto2010} for an general
introduction). Recent optimised experiments result in high quality
data enabling to identify over 7000 proteins and discriminate numerous
sub-cellular and sub-organellar niches. Supervised and semi-supervised
machine learning algorithms can be applied to assign thousands of
proteins to annotated sub-cellular niches. These data constitute our
main source for protein localisation and are termed thereafter
\emph{primary} data.

There are other sources of data about sub-cellular localisation of
proteins, such as the Gene Ontology \cite{Ashburner:2000} (in
particular the cellular compartment name space), quantitative feature
derived from protein sequences or the Human Protein Atlas
\cite{Uhlen:2010} for cite a few. These data, while not optimised to a
specific system at hand and, in the case of annotation feature, not as
reliable as our experimental data, constitute an invaluable, often
plentiful source of \emph{auxiliary} information.

The aim of a \emph{transfer learning} algorithm is to combine
different sources of data to improve overall classification. In
particular, the goal is to support/complement the primary target
domain (experimental data) with auxiliary data (annotation) features
without compromising the integrity of our primary data. In this
vignette, we describe the application of transfer learning algorithms
for the localisation of proteins from the \Biocpkg{pRoloc} package.

<<>>=
library("pRoloc")
@

\section{Preparing the auxiliary data}\label{sec:aux}

\subsection{The Gene Ontology}\label{sec:goaux}

The auxiliary data is prepared from the primary data's features. All
the GO terms associated to these features are retrieved and used to
create a binary matrix where a one in position $(i,j)$ indicates that
term $j$ has been used to annotate feature $i$.

The GO terms are retrieved from an appropriate repository using the
\Biocpkg{biomaRt} package. The specific Biomart repository and query
will depend on the species under study and the type of features. The
first step is to prepare annotation parameters that will enable to
perform the query. The \Biocpkg{pRoloc} package provides a dedicated
infrastructure to prepare the query to the annotation resource and
prepare the GO data for subsequent analyses. This infrastructure is
composed of:

\begin{enumerate}
\item prepare the annotation parameters based on the species and
  feature types;
\item query the resource defined in (1) and retrieve the terms;
\item use the terms to prepare the auxiliary data.
\end{enumerate}

We will describe how to perform step 2 above, although in practice,
steps 2 and 3 are combined into one single function. 

We will demonstrate these steps with 2 workding examples. The first
one is a LOPIT experiment on \textit{Arabodopsis thaliana} by
\cite{Dunkley2006}. The second data is a LOPIT experiment on Human
Embryonic Kidney (HEK293T) fibroblast cells \cite{Breckels2013}. Both
datasets are available and documented in the \Biocexptpkg{pRolocdata}
experiment package as \Robject{dunkley2006} and \Robject{andy2011}.

<<loaddata>>=
library("pRolocdata")
data(dunkley2006)
data(andy2011)
@

\subsubsection{Preparing the query parameters}\label{sec:ap}

The query parameters are stored as \Rclass{AnnotationParams} object
that be created with the \Rfunction{setAnnotationParams} function. The
function will present a first menu with
\Sexpr{nrow(pRoloc:::getMartTab())}. Once the species has been
selected, a set of possible identifier types is displayed. 


\begin{figure}[h]
  \centering
  \includegraphics[height=5cm]{./Figures/ap1.png}\hspace{1cm}
  \includegraphics[height=5cm]{./Figures/ap2.png}
  \caption{Selecting species (left) and feature type (right) to create
    an \Robject{AnnotationParams} instance for the human
    \Robject{andy2011} data.}
  \label{fig:apgui}
\end{figure}

It is also possible to pass patterns\footnote{These patterns must
  match uniquely or an error will be thrown.} to match against the
species (\texttt{"Homo sapiens"}) and feature type
(\texttt{"UniProt/Swissprot ID"}).

<<ap>>=
ap <- setAnnotationParams(inputs =
                              c("Homo sapiens",
                                "UniProt/Swissprot ID"))
ap
@

The \Rfunction{setAnnotationParams} function sets the annotation
parameters globally so that the \Robject{ap} object does not neet to
be explicitly set later on. The default parameters can be retrieved
with \Rfunction{getAnnotationParams}. 

Below is another example using \textit{Arabodopsis thaliana} and the
TAIR locus IDs. 

<<ap2>>=
ap2 <- setAnnotationParams(inputs =
                               c("Arabidopsis thaliana",
                                 "TAIR locus ID"))
ap2
@

\subsubsection{Preparing the auxiliary data from the GO ontology}\label{sec:auxgo}

The \Rfunction{goGoFromFeatures} takes an \Rclass{MSnSet} class (from
which the feature names will be extracted) or, directly a vector of
characters containing the feature names of interest to retrieve the
associated GO terms. By default, it downloads \textit{cellular
  component} terms and does not do any filtering on the terms evidence
codes (see the \Rfunction{getGOFromFeatures} manual for
details). Unless passed as argument, the default, globally set
\Rclass{AnnotationParams} are used to define the Biomart server and
the query.

\paragraph*{The \Robject{dunkley2006} data}

Below, we show an example on how we can retrieve the GO terms for the
\Robject{dunkley2006} dataset \cite{Dunkley2006}, available in the
\Biocexptpkg{pRolocdata} package. We load the data and verify that the
feature names correspond the the TAIR locus identifiers.

<<dun>>=
data(dunkley2006)
head(featureNames(dunkley2006))
@

<<dungo>>=
dunkleygo <- getGOFromFeatures(dunkley2006, params = ap2)
head(dunkleygo)
@

We could also have passed the vector for feature names:

<<dungo2, eval=FALSE>>=
getGOFromFeatures(featureNames(dunkley2006), params = ap2)
@

We now have, in the \Robject{dunkleygo}, the GO terms that have been
associated to the feature names identified in the
\Robject{dunkley2006} dataset. Instead of tuning this information into
an \Robject{MSnSet} manually, we can use the \Rfunction{makeGoSet}
function.

<<dungoset>>=
dunkleygoset <- makeGoSet(dunkley2006, params = ap2)
dunkleygoset
exprs(dunkleygoset)[1:7, 1:4]
@

<<testdunsamefeats, echo=FALSE>>=
stopifnot(all.equal(featureNames(dunkley2006), featureNames(dunkleygoset)))
@


The \Robject{dunkley2006} object stores \Sexpr{nrow(dunkley2006)}
quantitative protein localisation profiles along
\Sexpr{ncol(dunkley2006)} gradient fractions and the
\Robject{dunkleygoset} stores a binary association matrix for
\Sexpr{nrow(dunkleygoset)} and \Sexpr{ncol(dunkleygoset)} GO terms.


\paragraph*{The \Robject{andy2011} data}

The situation for the \Robject{andy2011} data is slightly
different. As can be seen in the next code chunk, the feature names do
not match exactly any of the feature types available (see figure
\ref{fig:apgui} on the right). 

<<and>>=
data(andy2011)
head(featureNames(andy2011))
@

However, the UniProt/SwissProt identifiers are available in the
feature data:

<<ids>>=
head(fData(andy2011)$Accession.No.)
@

As these are unique (feature names must be), we are going to (1) store
the current feature names as a new feature varaiable, named
\texttt{UniProtKB.entry.name}, and use the \texttt{Accession.No.}
feature variable as new feature names.

<<entry>>=
fData(andy2011)$UniProtKB.entry.name <- featureNames(andy2011)
featureNames(andy2011) <- fData(andy2011)$Accession.No.
@

We first set the appropriate annotation parameters \Robject{ap}
generated in section \ref{sec:ap} (we could also pass them explicitely
as in the previous example) and use the \Rfunction{makeGoSet} function
to create the binary auxiliary data.

<<andgoset>>=
setAnnotationParams(ap)
andygoset <- makeGoSet(andy2011)
andygoset
exprs(andygoset)[1:7, 1:4]
@

<<testandsamefeats, echo=FALSE>>=
stopifnot(all.equal(featureNames(andy2011), featureNames(andygoset)))
@

We now have a primary dataset, composed of \Sexpr{nrow(andy2011)}
protein quantitative profiles for \Sexpr{ncol(andy2011)} fractions
along the density gradient and an auxiliary dataset for
\Sexpr{ncol(andygoset)} cellular compartment GO terms for the same
\Sexpr{nrow(andygoset)} features.

\subsubsection{A note on reproducibility}\label{sec:annotrepro}

The generation of the auxiliary data replies on specific Biomart
server \Rclass{Mart} instances in the \Rclass{AnnotationParams} class
and the actual query to the server to obtain the GO terms associated
with the features. The utilisation of the online servers, which
undergo regular updates, does not guarantee reproducibility of
feature/term association over time. It is recommended to save and
store the \Rclass{AnnotationParams} and auxiliary \Rclass{MSnSet}
instances. Alternatively, I should be possible to use other
Bioconductor infrastructure, such as specific organism annotations and
the \Biocannopkg{GO.db} package to use specific versioned (and thus
traceable) annotations.

\subsection{The Human Protein Atlas}\label{sec:hpaaux}

The feature names of our LOPIT experiment are UniProt identifiers,
while the Human Protein Atlas uses Ensembl gene identifiers. This
first code chunk matches both identifier types using the UniProt
Biomart server.

<<hparprep, eval=TRUE>>=
fvarLabels(andy2011)[1] <- "accession" ## for left_join matching

## convert protein accession numbers to ensembl gene identifiers
library("biomaRt")
uniprot <- useMart("unimart", dataset = "uniprot")
filter <- "accession"
attrib <- c("name", "accession", "ensembl_id")
bm <- getBM(attributes = attrib,
            filters = filter,
            values = fData(andy2011)[, "accession"],
            mart = uniprot)

## HPA data
library("hpar")
setHparOptions(hpadata = "SubcellularLoc")
hpa <- getHpa(bm$ensembl_id)
hpa$Reliability <- droplevels(hpa$Reliability)
colnames(hpa)[1] <- "ensembl_id"

library("dplyr")
hpa <- left_join(hpa, bm)
hpa <- hpa[!duplicated(hpa$accession), ]

## match HPA/LOPIT
fd <- left_join(fData(andy2011), hpa)
rownames(fd) <- featureNames(andy2011)
fData(andy2011) <- fd
stopifnot(validObject(andy2011))

## Let's get rid of features without any hpa data
lopit <- andy2011[!is.na(fData(andy2011)$Main.location), ]
@

Below, we deparse the multiple ';'-delimited locations contained in
the Human Protein sub-cellular Atlas, create the auxiliary binary data
matrix (only localisations with reliability equal to \emph{Supportive}
are considered; \emph{Uncertain} assignments are ignored) and filter
proteins without any localisation data.

<<hpadata, eval=TRUE>>=
## HPA localisation
hpalocs <- c(as.character(fData(lopit)$Main.location),
             as.character(fData(lopit)$Other.location))
hpalocs <- hpalocs[!is.na(hpalocs)]
hpalocs <- unique(unlist(strsplit(hpalocs, ";")))

makeHpaSet <- function(x, score2, locs = hpalocs) {
    hpamat <- matrix(0, ncol = length(locs), nrow = nrow(x))
    colnames(hpamat) <- locs
    rownames(hpamat) <- featureNames(x)    
    for  (i in 1:nrow(hpamat)) {
        loc <- unlist(strsplit(as.character(fData(x)[i, "Main.location"]), ";"))
        loc2 <- unlist(strsplit(as.character(fData(x)[i, "Other.location"]), ";"))
        score <- score2[fData(x)[i, "Reliability"]]
        hpamat[i, loc] <- score
        hpamat[i, loc2] <- score
    }
    new("MSnSet", exprs = hpamat,
        featureData = featureData(lopit))
}

hpaset <- makeHpaSet(lopit,
                     score2 = c(Supportive = 1, Uncertain = 0))
hpaset <- filterZeroRows(hpaset)
dim(hpaset)
exprs(hpaset)[c(1, 6, 200), 1:3]
@

\section{Calculating weights}\label{sec:thopt}

<<>>=
m <- unique(fData(andy2011)$markers)
m <- m[m != "unknown"]
@

The weighted nearest neighbours transfer learning algorithm estimates
optimal weights for the different data sources and the spatial niches
described for the data at hand. For instance, for the human data
modelled by the \Robject{andy2011} and \Robject{andugoset} objects and
the \Sexpr{length(m)} annotated sub-cellular localisations
(\Sexpr{paste(m[-1], collapse = ", ")} and \Sexpr{m[1]}), we want to
know how to optimally use the different data sources. If we look at
figure \ref{fig:andypca}, that illustrates the experimental separation
of the \Sexpr{m} spatial classes on a principal component plot, we see
that some organelles such as the mitochondrion or the cytosol and
cytosol/nucleus are well resolved, while others such as the Golgi or
the ER are less so. In this experiment, the former classes are not
expected to benefit for other data source, while the latter should
benefit from additional information.

\begin{figure}[h]
  \centering
<<andypca, echo=FALSE>>=
setStockcol(paste0(getStockcol(), "80"))
plot2D(andy2011)
setStockcol(NULL)
addLegend(andy2011, where = "topright", bty = "n", cex = .8)
@  
 \caption{PCA plot of \Robject{andy2011}. The multivariate protein
   profiles are summerised along the two first principal
   components. Proteins of unknown localisation are represented by
   empty grey points. Protein markers, which are well-known residents
   of specific sub-cellular niches are colour-coded and form clusters
   on the figure. }
  \label{fig:andypca}
\end{figure}


Let's define a set of three possible weights: 0, 0.5 and 1. A weight
of 1 indicates that the final results rely exclusively on the
experimental data and ignore completely the auxiliary data. A weight
of 0 represents the opposite, where the primary data is ignored and
only the auxiliary data is considered. A weight of 0.5 indicates that
each data source will contribute equally to the final results.  It is
the algorithm's optimisation step to identify the optimal set of
weights for a given primary and auxiliary data pair. The optimisation
process can be quite time consuming for many weights and many
sub-cellular classes, as all combinations (there are
$number of classes^{number of weights}$ possibilities; see below). One would
generally defined more weights (for example \Sexpr{seq(0, 1, by =
  0.25)} or \Sexpr{round(seq(0, 1, length.out = 4), 2)}) to explore
more fine-grained integration opportunities. The possible weight
combinations can be calculated with the \Rfunction{thetas} function:

<<thetas>>=
## 3 classes, 3 weights
head(thetas(3, by = 0.5))
dim(thetas(3, by = 0.5))
## 3 classe, 3 weights
dim(thetas(3, length.out = 4))
## 10 classe, 5 weights
dim(thetas(10, length.out = 5))
@

Considering the sub-cellular resolution for this experiment, we would
anticipate that the mitochondrion, the cytosol and the cytosol/nucleus
classes would get high weights, while the ER and Golgi would be
assigned lower weights.

As we use a nearest neighbour classifier, we also need to know how
many neighbours to consider when classifying a protein of unknown
localisation. The \Rfunction{knnOptimisation} function can be run on
the primary and auxiliary data sources independently to estimate the
best $k_P$ and $k_A$ values. Below, we use 3 and 3.

Below, pass a small subset of theta matrix to make it fast.

<<thetaopt, eval=FALSE>>=

topt <- thetaOptimisation(andy2011, andygoset,
                          length.out = 4, ## weights are seq(0, 1, length.out = 4)
                          k = c(3, 3),
                          times = 20)
@

\section{Applying best \textit{theta} weights}\label{sec:thclass}

\subsection{Choosing weights}

With \Rfunction{getParams} or manually, by inspecting the weight plots.

Favouring primary data.

\section{Conclusion}\label{sec:ccl}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Session information}\label{sec:sessionInfo} 

All software and respective versions used to produce this document are
listed below.

<<sessioninfo, results='asis', echo=FALSE>>=
toLatex(sessionInfo())
@

\bibliography{pRoloc}

\end{document}

