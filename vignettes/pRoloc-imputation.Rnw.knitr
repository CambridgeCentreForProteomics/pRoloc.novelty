%\VignetteIndexEntry{Missing data imputation}
%\VignetteKeywords{Bioinformatics, Machine learning, Organelle, Proteomics}
%\VignettePackage{pRoloc}
\documentclass[12pt]{article}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage[authoryear,round]{natbib}
\usepackage[auth-sc]{authblk}
\usepackage{setspace}
\onehalfspacing

\renewcommand\Authands{ and }

\newcommand{\R}{\texttt{R} }
\newcommand{\code}[1]{{\texttt{#1}}}
\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\mbox{\normalfont\textsf{#1}}}}
\newcommand{\email}[1]{\href{mailto:#1}{\normalfont\texttt{#1}}}
%% colors
\definecolor{Red}{rgb}{0.7,0,0}
\definecolor{Blue}{rgb}{0,0,0.8}

\usepackage{geometry}
\geometry{verbose,
  tmargin = 2.5cm,
  bmargin = 2.5cm,
  lmargin = 3.0cm,
  rmargin = 3.0cm}

\usepackage{hyperref}
\usepackage{breakurl}
\hypersetup{%
  pdfusetitle,
  bookmarks = {true},
  bookmarksnumbered = {true},
  bookmarksopen = {true},
  bookmarksopenlevel = 2,
  unicode = {true},
  breaklinks = {false},
  hyperindex = {true},
  colorlinks = {true},
  linktocpage = {true},
  plainpages = {false},
  linkcolor = {Blue},
  citecolor = {Blue},
  urlcolor = {Red},
  pdfstartview = {Fit},
  pdfpagemode = {UseOutlines},
  pdfview = {XYZ null null null}
}


\author{
  Laurent Gatto\thanks{\email{lg390@cam.ac.uk}}
}

\affil{
  Cambridge Center for Proteomics\\
  University of Cambridge
}


\begin{document}

\title{Data imputation on spatial proteomics data}

\maketitle

%% Abstract and keywords %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vskip 0.3in minus 0.1in
\hrule
\begin{abstract}
This document illustrates the some procedures that can be use as a quality control when performing data imputation, with special emphasis on spatial/organelle proteomics data using the \Rpackage{pRoloc} package.
\end{abstract}
\textit{Keywords}: Bioinformatics, spatial/organelle proteomics, machine learning, visualisation
\vskip 0.1in minus 0.05in
\hrule
\vskip 0.2in minus 0.1in
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

\tableofcontents

<<env, include=FALSE, echo=FALSE, cache=FALSE>>=
library("knitr")
opts_chunk$set(fig.align = 'center', 
               fig.show = 'hold', 
               par = TRUE,
               prompt = TRUE,
               eval = TRUE,
               comment = NA)
options(replace.assign = TRUE, 
        width = 55)

suppressPackageStartupMessages(library("MSnbase"))
suppressWarnings(suppressPackageStartupMessages(library("pRoloc")))
suppressPackageStartupMessages(library("pRolocdata"))
suppressPackageStartupMessages(library("class"))
suppressPackageStartupMessages(library("ggplot2"))
suppressPackageStartupMessages(library("reshape2"))
suppressPackageStartupMessages(library("xtable"))
@ 
%%$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

\section*{Foreword}

Please take into account that this document is work in progress.

\bigskip
\begin{itemize}
\item TODO -- use root mean square to compare methods
\item TODO -- compare different $k$ in $nn$-imputation.
\end{itemize}

\section{Introduction}\label{sec:intro} 

Data imputation is performed using the \Rfunction{impute} method, defined in the \Rpackage{MSnbase} package \citep{Gatto2012}. 
The default imputation method is nearest neighbour imputation \citep{Troyanskaya2001}.
The difficulty in performing data imputation is not the imputation itself, but making sure that the imputation procedure 
produces plausible results. These new values will be used for further data analysis and, eventually, interpretation of the 
underlying biology. As such, we advise users to assure that the imputation can be reasonably trusted and suggest some simple 
methods, mostly based on visualisation. 

For an introduction to \Rpackage{pRoloc}, readers are referred to the tutorial, 
available using \Rfunction{vignette("pRoloc-tutorial", package = "pRoloc")}.

<<loadpackages>>=
library("pRoloc")
library("pRolocdata")
library("reshape2")
library("ggplot2")
library("xtable")
@ 

\section{Data imputation}

\subsection{Preparing test data}\label{sec:testdata1}

We will use data distrubuted with the \Rpackage{pRolocdata} package, in particular from \cite{Tan2009} and, 
later in section \ref{sec:testdata2}, from \cite{Dunkley2006}. Please refer to the documentation of these 
respective data and publication for more details. Below, we first load the packages that will be used in 
this document.

<<setnNA, echo = FALSE>>=
data(tan2009r1)
nNA <- 180
pNA <- nNA / prod(dim(tan2009r1))
@ 

The \Rfunction{makeNaData} function sets \Robject{nNA} values to \Robject{NA} in a \Robject{MSnSet} instance. 
It is used to generate data sets with randomly distributed missing values.
In this case, we have set \Robject{nNA} to be \Sexpr{nNA}, which represemts \Sexpr{round(pNA, 2) * 100} \% of the data. 
This is still a relatively low proportion of missing data. 
In addition, we are going to make sure that the organelle markers, defined in 
\Robject{fData(tan2009r1)\$markers}
are going to be excluded when setting missing values. 
This is done to avoid biases in the procedure described in section \ref{sec:class}.

An important point to highlight at this stage is that the distribution 
of the missing values, as simulated data generated by \Rfunction{makeNaData}, 
is trully random. Non-random distribution of missing values should be 
addressed specifically, in the light of the experimental design.

<<testdata>>=
data(tan2009r1)
mrk <- fData(tan2009r1)$markers != "unknown"
nNA
set.seed(1) ## for reproducibility
tan2009NA <- makeNaData(tan2009r1, nNA, exclude = mrk)
processingData(tan2009NA)
table(fData(tan2009NA)$nNA)
@ 

\subsection{Data imputation}\label{sec:imp}

Below, we perform data imputation and compare the original values from \Robject{tan2009r1} to the 
imputed data from \Robject{tan2009imp}. The indices of the missing values are extracted with the
\Rfunction{whichNA} function and the $log_2$ ratios between the original and imputed values are 
computed and stored in the \Robject{r} variable.

<<impute>>=
tan2009imp <- impute(tan2009NA)
processingData(tan2009imp)
naIdx <- whichNA(tan2009NA)
r <- log2(exprs(tan2009r1)[naIdx]/exprs(tan2009imp)[naIdx])
summary(r)
@ 

\section{Exploring imputation}

\subsection{Exploring missing data}

See \Rfunction{MSnbase::plotNA} and other plots in old missing data vignette.

\subsection{Patterns of missing data and imputation}

We first prepare a \Robject{data.frame}, \Robject{rdf}, that stores the 
$\frac{original}{imputed}$ $log_2$ ratios, calculated in section \ref{sec:imp}. 
For visualisation purposes, the \Robject{rdf2} variable retains only 
features that have missing values.

<<rdf, tidy = FALSE>>=
rdf <- matrix(NA, 
              ncol = ncol(tan2009r1), 
              nrow = nrow(tan2009r1))
colnames(rdf) <- sampleNames(tan2009r1)
rdf[naIdx] <- r
rdf2 <- rdf[!apply(rdf, 1, function(x) all(is.na(x))), ]
rdf <- melt(rdf)
rdf2 <- melt(rdf2)
colnames(rdf2) <- colnames(rdf) <- c("y", "x", "z")
@ 

Below, we use \Rpackage{ggplot2} to colour code the ratios in the 
expression matrix. These figures illustrate the extend of the effect 
of imputation (divergence of the $log_2$ ratios from 0) as well as 
the distribution of the missing values in the expression matrix, 
inviting to reflect on the relation between possible patterns 
of missing data and their effect on imputation.
This visualisation should be particularly informative when the 
missing data is non random accross the quantitation matrix.

\begin{figure}[!hbt]
<<tileplot, fig.width=4, fig.height=6, out.width='.4\\linewidth', fig.keep='high', tidy = FALSE>>=
colscale <- c("green", "yellow", "red")
legtitle <- expression(log[2](frac(Orgiginal, Imputed)))
p <- ggplot(rdf, aes(x = x, y = y, fill = z))
p2 <- ggplot(rdf2, aes(x = x, y = y, fill = z))
p <- p + geom_tile() + theme_bw() + 
  scale_fill_gradientn(colours=colscale, na.value = NA,
                       name = legtitle)
p2 <- p2 + geom_tile() + theme_bw() + 
    scale_fill_gradientn(colours=colscale, na.value = NA,
                       name = legtitle)
print(p)
print(p2)
@ 
\caption{Position of the missing values in the data matrix. The coloured tiles 
represent the $\frac{original}{imputed}$ ratios. The figure on the right only
shows features with missing values. }
\label{fig:tile}
\end{figure}


\subsection{Effect of imputation on cluster resolution}\label{sec:clusterres}

Visualisation of spatial proteomics data on a 2 dimensional plot using 
dimensionality reduction techniques such as principal component analysis
is enlightening in terms of quality control, as they provide a direct 
reflection on separation the cellular content and the ability to assign 
sub-cellular localisation. In this section, we make use of the 
\Rfunction{plot2D} function from \Rpackage{pRoloc}, to visualist the effect 
of imputation.

\begin{figure}[!htp]
<<plot2dimp, fig.width=8, fig.height=4, out.width='1\\linewidth', fig.keep='high', tidy = FALSE>>=
setUnknowncol("black")
par(mfrow = c(1,2))
## plot 1
plot2D(tan2009r1, cex = fData(tan2009NA)$nNA + 0.25)
title(main = "Original")
## plot 2
plot2D(tan2009imp, cex = fData(tan2009NA)$nNA + 0.25)
title(main = "Imputed")
@ 
\caption{Each figure represents the graident quantitation data as a 2 dimensional PCA plot. 
  Coloured points are organelle specific markers. We have used the number of missing values 
  per feature \Robject{fData(tan2009NA)\$nNA} to adjust the point size; the larger the 
  point, the more missing values were imputed for that feature. 
  The tiny points were complete while one feature, represented by the biggest circle, 
  had two missing values.  
}
\label{fig:plot2dimp}
\end{figure}

To fully appreciate the effect of the imputation on the data, we are going to 
combine the original and imputed data on the same figure.

<<prepdata1>>=
sel <- which(fData(tan2009imp)$nNA > 0)
d1 <- plot2D(tan2009r1, plot = FALSE)
d2 <- plot2D(tan2009imp, plot = FALSE)
d1 <- d1[sel, ]; d2 <- d2[sel, ]
xlim <- range(range(d1[, 1]), range(d2[, 1]))
ylim <- range(range(d1[, 2]), range(d2[, 2]))
@ 


<<prepdata2>>=
p1 <- exprs(tan2009r1)[sel, ]
p2 <- exprs(tan2009imp)[sel, ]
impdists <- sapply(1:nrow(p1),
                   function(i) dist(rbind(p1[i, ], p2[i, ])))
ddf <- data.frame(nNA = factor(fData(tan2009NA)$nNA[sel]),
                  distance = impdists)
rownames(ddf) <- rownames(p1)
pp <- qplot(nNA, distance, data = ddf, geom = "boxplot")
@ 

\begin{figure}[!htp]
<<plot2dimp2, out.width='.45\\linewidth', fig.keep='high', tidy = FALSE>>=
plot(d1[, 1], d1[, 2], pch = 1, 
     cex = fData(tan2009NA)$nNA[sel],
     xlab = "PC1", ylab = "PC2", 
     xlim = xlim, ylim = ylim,
     main = "Effect of imputation")
grid()
points(d2[, 1], d2[, 2], pch = "+", 
       cex = fData(tan2009NA)$nNA[sel])
segments(d1[, 1], d1[, 2], 
         d2[, 1], d2[, 2],
         col = "grey50")
print(pp)
@ 
\caption{On this figure, we represent the imputed features only. 
  The circles represent the location of the original values and 
  crosses represent positions after imputation. 
  On the right, we show the \textit{imputation distances}, i.e. 
  the effect of imputation on the features position in the 
  feature space for single or double value imputations.}
\label{fig:plot2dimp2}
\end{figure}

The effect of imputation can not be applied on real missing data 
as discribed above. Visualisation on the PCA plot, and 
\Rfunction{plot2D} can still be applied on the complete and the 
imputed data.

\clearpage

\subsection{Effect of imputation on classification}\label{sec:class}

In this section, we are going to assess the effect of imputation 
on the classification itself. 

\bigskip

In section \ref{sec:testdata1} (page \pageref{sec:testdata1}), we have 
made sure that none of the organelle markers had been set a missing value. 
This has as implication that 
(1) the parameter optimisation, which 
relies solely on the labelled markers, will be the same for any imputed, 
original or filtered (removing all features that undergone imputation)
data and, as such, results in the same model parameter values and 
(2) as the markers and parameters 
are identical, the support vector machine classifier that we use will 
assign non imputed features the same classes (in other words, 
the classification depends on the original labelled data and the model parameters only). 
The classification scores will also be identical. 
The only difference between the above cases will be the 
classification of the imputed features. 

As a result, we will perform the classification on the imputed 
and the original data sets, and concentrate on the feature that 
undergone imputation (see figure \ref{fig:plot2dimp2}). 

\subsubsection{Classification using support vector machines}

<<loadpar, echo = FALSE>>=
load(dir(system.file("extdata", package = "pRoloc"), 
         full.names = TRUE, pattern = "param.rda"))
@ 

Before proceeding with the actual classification, we optimise the 
\Robject{sigma} and \Robject{cost} parameters using 
\Rfunction{svmOptimisation}\footnote{%%
In the interest of time, \Rfunction{svmOptimisation} is not executed when the 
vignette is dynamically built. The \Robject{par} object can be located with 
\Rfunction{dir(system.file("extdata", package = "pRoloc"), full.names = TRUE, pattern = "par.rda")} 
and loaded with \Rfunction{load}.}%%
(see \Rfunction{?svmOptimisation} and the tutorail vignette for details). 


<<par1, tidy = FALSE, eval = FALSE>>=
params <- svmOptimisation(tan2009imp, verbose = FALSE)
@ 

Here, we do not detail the selection of the best parameters using \Rfunction{plot(par1)}, 
\Rfunction{levelPlot(par1)} and \Rfunction{f1Count(par1)}. We have checked that the 
best values used by default (those selected by \Rfunction{getParams(par1)}) were appropriate.

We then apply a standard supervised classification procedure using 
support vector machines, as implemented in \Rpackage{pRoloc} 
(see \Rfunction{?svmClassification} and the tutorial vignette for more details) 
on the imputed and original data and extract the features of interest.

<<cl1, tidy = FALSE, warning = FALSE>>=
impft <- which(fData(tan2009imp)$nNA > 0)
clImp <- svmClassification(tan2009imp, params)[impft, ]
clOrg <- svmClassification(tan2009r1, params)[impft, ]
@ 

\subsubsection{Comparison of the classification results}

Below, we compare the classifcation results obtained above. 
We observe one case that has been classified as PM in the 
original data set and ER after imputation. The other 6 
disagreeing cases have been classified as PM after imputation, 
while they were either ER, Golgi or mitochondrion orifinally. 

<<comp1>>=
table(Original = fData(clOrg)$svm, 
      Imputed = fData(clImp)$svm)
@

<<morecomp, tidy = FALSE>>=
diffcl <- which(fData(clImp)$svm != fData(clOrg)$svm)
diffcl <- featureNames(clImp)[diffcl]
tab <- cbind(ddf[diffcl, ], 
             Original = fData(clOrg)[diffcl, "svm"], 
             score = fData(clOrg)[diffcl, "svm.scores"],       
             Imputed = fData(clImp)[diffcl, "svm"],
             scores = fData(clImp)[diffcl, "svm.scores"])
colnames(tab) <- c("Number nof NAs", "Imputation distance", 
                   "Orignal class", "Orignal score",
                   "Imputed class", "Imputed score")
@ 

We illustrate these conflicting features in figure \ref{fig:compclass}
and table \ref{tab:compclass}. Refering back to figure \ref{fig:plot2dimp2}, 
these features have greater imputation distances. 

Most noteworthy are features \Sexpr{diffcl[2]} and \Sexpr{diffcl[6]}, 
which have relatively high imputed classification scores and would be reasonably trusted. 
We can observe on figure \ref{fig:compclass} how 
imputation moves them from a zone of low classification reliability 
to the PM cluster. 

\begin{figure}[!htp]
<<compclass, out.width='.65\\linewidth', fig.keep='high', tidy = FALSE>>=
plot(d1[, 1], d1[, 2], pch = 1, 
     cex = fData(tan2009NA)$nNA[sel],
     xlab = "PC1", ylab = "PC2", 
     xlim = xlim, ylim = ylim,
     col = ifelse(rownames(d1) %in% diffcl, 
       "red", "black"),
     main = "Effect of imputation")
text(d1[diffcl, 1], d1[diffcl, 2], 
     labels = 1:6, col = "red", 
     adj = c(-0.3, 1.6))
grid()
points(d2[, 1], d2[, 2], pch = "+", 
       cex = fData(clImp)$nNA, 
       col = ifelse(rownames(d1) %in% diffcl, 
       "red", "black"))
segments(d1[, 1], d1[, 2], 
         d2[, 1], d2[, 2],
         col = ifelse(rownames(d1) %in% diffcl, 
           "red", "grey50"), 
         lty = "dotted")
@ 
\caption{Features with imputed values and how imputation effects 
their position in the feature space. The \Sexpr{length(diffcl)} 
features that differ in classifictation between the original and 
imputed data are highlighted in red. The numbers correspond to the 
order in table \ref{tab:compclass}. }
\label{fig:compclass}
\end{figure}


<<comptab, results='asis', echo=FALSE>>=
tab$'Orignal class' <- sub("mitochondrion", "Mito", tab$'Orignal class')
xt <- xtable(tab,
             label="tab:compclass",
             caption = "Details of the features showing different classification after imputation.")
align(xt) <- rep('p{.65in}', ncol(tab) + 1)
print(xt)   
@ 

\clearpage

\subsection{An iterative approach}

\subsubsection{Another test data}\label{sec:testdata2}

<<pretestdata2, echo=FALSE>>=
data(dunkley2006)
prep1 <- dunkley2006$membrane.prep == 1
mrk <- fData(dunkley2006)$markers != "unknown"
set.seed(1)
dunkleyNA <- makeNaData2(dunkley2006[, prep1],
                         nRows = rep(35, 5),
                         nNAs = 1:5, 
                         exclude = mrk)
@ 

In this case, we use only the first membrane preparation of the \cite{Dunkley2006} data set. 
Here, we use\Rfunction{makeNaData2}, which allows to specify the number of missing values to 
be added for a certain number of lines. We choose to set 1 to 5 \texttt{NA} values to 
35 features respectively, avoiding the organelle markers. 
In total, we end up with \Sexpr{round(100 * sum(is.na(dunkleyNA))/prod(dim(dunkleyNA)), 1)}\% 
of missing values over the full data set.

When performing imputation, the algorithm informs that 35 rows have more than 50\% of missing 
values, as defined in out missing data strategy, in which cases, mean imputation has been applied.

<<testdata2>>=
data(dunkley2006)
prep1 <- dunkley2006$membrane.prep == 1
mrk <- fData(dunkley2006)$markers != "unknown"
set.seed(1)
dunkleyNA <- makeNaData2(dunkley2006[, prep1],
                         nRows = rep(35, 5),
                         nNAs = 1:5, 
                         exclude = mrk)
processingData(dunkleyNA)
table(fData(dunkleyNA)$nNA)
dunkleyImp <- impute(dunkleyNA)
@ 

The original data set and the markers are illustrated on figure \ref{fig:dunkley}.

\begin{figure}[!htp]
<<dunkley, out.width='.55\\linewidth', fig.keep='high', tidy = FALSE>>=
plot2D(dunkley2006, main = "Original Dunkey 2006 data")
addLegend(dunkley2006, where = "bottomright", 
          bty = "n", ncol = 6, cex = .75)
@ 
\caption{ PCA plot of the original \Robject{dunkley2006} data set. }
\label{fig:dunkley}
\end{figure}

We first illustrate the effect of imputation on the complete data on figure 
\ref{fig:distdunkley}. The effect of the number of missing values seems 
to be distinctive for 4 and 5 (out of \Sexpr{ncol(dunkleyImp)} imputations. 
The PCA plot seems to indicate a concentration of points toward the 
centre of the plot as a result of imputation. 

\begin{figure}[!htp]
<<distdunkley, out.width='.49\\linewidth', fig.keep='high', echo = FALSE>>=
d1 <- plot2D(dunkley2006, plot = FALSE)
d2 <- plot2D(dunkleyImp, plot = FALSE)
sel <- which(fData(dunkleyImp)$nNA > 0)
cls <- paste0(c("#999999",
                getStockcol()[1:5]),
              80)
plot(d1, col = cls[fData(dunkleyImp)$nNA + 1], 
     pch = ifelse(fData(dunkleyImp)$nNA == 0,
       1, 19)) 
grid()
x <- unique(fData(dunkleyImp)$nNA)
arrows(d1[sel, 1], d1[sel, 2], 
       d2[sel, 1], d2[sel, 2],
       col = "grey50", length = 0.1,
       lty = "dotted")
legend("bottom", bty = "n", ncol = 6, 
       legend = sort(x), 
       col = cls[order(x)], pch = c(1, rep(19, 5)))

p1 <- exprs(dunkley2006)[sel, ]
p2 <- exprs(dunkleyImp)[sel, ]
impdists <- sapply(1:nrow(p1),
                   function(i) dist(rbind(p1[i, ], p2[i, ])))
ddf <- data.frame(nNA = factor(fData(dunkleyImp)$nNA[sel]),
                  distance = impdists)
rownames(ddf) <- rownames(p1)
pp <- qplot(nNA, distance, data = ddf, geom = "boxplot")
print(pp)
@ 
\caption{ Effect of imputation on the dunkley data set.  }
\label{fig:distdunkley}
\end{figure}

\subsubsection{Iterative imputation}

Instead of imputing all values at once, we are going to impute features 
with 1 missing value, then use this updated data to impute features with 
2 missing values, \ldots and see if this approach leads to any improvements.

<<iterimp>>=
nas <- lapply(0:5, function(k)
              which(fData(dunkleyNA)$nNA == k))

## Impute features with 1 NA
dunkleyImp1 <- impute(dunkleyNA[unlist(nas[1:2])])
## Impute features with 2 NA
dunkleyImp2 <- impute(combine(dunkleyImp1, dunkleyNA[nas[[3]]]))
## Impute features with 3 NA
dunkleyImp3 <- impute(combine(dunkleyImp2, dunkleyNA[nas[[4]]]))
## Impute features with 4 NA
dunkleyImp4 <- impute(combine(dunkleyImp3, dunkleyNA[nas[[5]]]))
## Impute features with 5 NA
dunkleyImp5 <- impute(combine(dunkleyImp4, dunkleyNA[nas[[6]]]))
@ 


\begin{figure}[!htp]
<<distdunkley2, out.width='.49\\linewidth', fig.keep='high', echo = FALSE, warning = FALSE>>=

dunkleyImp5 <- dunkleyImp5[featureNames(dunkley2006)]
d1 <- plot2D(dunkley2006, plot = FALSE)
d2 <- plot2D(dunkleyImp5, plot = FALSE)
sel <- which(fData(dunkleyImp5)$nNA > 0)
plot(d1, col = cls[fData(dunkleyImp5)$nNA + 1], 
     pch = ifelse(fData(dunkleyImp5)$nNA == 0, 1, 19),
     main = "Iterative imputation vs. original data") 
grid()
arrows(d1[sel, 1], d1[sel, 2], 
       d2[sel, 1], d2[sel, 2],
       col = "grey50", length = 0.1,
       lty = "dotted")
legend("bottom", bty = "n", ncol = 6, 
       legend = sort(x), 
       col = cls[order(x)], pch = c(1, rep(19, 5)))

d1 <- plot2D(dunkleyImp, plot = FALSE)
d2 <- plot2D(dunkleyImp5, plot = FALSE)
sel <- which(fData(dunkleyImp5)$nNA > 0)
plot(d1, col = cls[fData(dunkleyImp5)$nNA + 1], 
     pch = ifelse(fData(dunkleyImp5)$nNA == 0, 1, 19),
     main = "Iterative imputation vs. complete imputation.") 
grid()
arrows(d1[sel, 1], d1[sel, 2], 
       d2[sel, 1], d2[sel, 2],
       col = "grey50", length = 0.1,
       lty = "dotted")
legend("bottom", bty = "n", ncol = 6, 
       legend = sort(x), 
       col = cls[order(x)], pch = c(1, rep(19, 5)))

@ 
\caption{ Effect of iterative imputation on the dunkley data set.  }
\label{fig:distdunkley2}
\end{figure}



\singlespacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage

\section*{Session information}\label{sec:sessionInfo} 

All software and respective versions used to produce this document are listed below.

<<sessioninfo, results='asis', echo=FALSE>>=
toLatex(sessionInfo())
@

\bibliographystyle{plainnat}
\bibliography{pRoloc}

\end{document}

