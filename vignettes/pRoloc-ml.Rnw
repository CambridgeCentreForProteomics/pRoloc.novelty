%\VignetteIndexEntry{Machine learning techniques available in pRoloc}
%\VignetteKeywords{Bioinformatics, Machine learning, Organelle, Proteomics}
%\VignettePackage{pRoloc}
\documentclass[12pt,a4paper,english]{scrartcl}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{tikz}
\usepackage{hyperref}
\usepackage[authoryear,round]{natbib}
\usepackage[auth-sc]{authblk}
\usepackage{setspace}
\onehalfspacing

% caption formatting
\setcapindent{0em}
\setkomafont{captionlabel}{\sffamily\bfseries}
\setkomafont{caption}{\sffamily}

\renewcommand\Authands{ and }

\newcommand{\R}{\texttt{R} }
\newcommand{\code}[1]{{\texttt{#1}}}
\newcommand{\Rfunction}[1]{{\texttt{#1}}}
\newcommand{\Robject}[1]{{\texttt{#1}}}
\newcommand{\Rpackage}[1]{{\mbox{\normalfont\textsf{#1}}}}
\newcommand{\email}[1]{\href{mailto:#1}{\normalfont\texttt{#1}}}
%% colors
\definecolor{Red}{rgb}{0.7,0,0}
\definecolor{Blue}{rgb}{0,0,0.8}

\usepackage{geometry}
\geometry{verbose,
  tmargin = 2.5cm,
  bmargin = 2.5cm,
  lmargin = 3.0cm,
  rmargin = 3.0cm}

\usepackage{hyperref}
\usepackage{breakurl}
\hypersetup{%
  pdfusetitle,
  bookmarks = {true},
  bookmarksnumbered = {true},
  bookmarksopen = {true},
  bookmarksopenlevel = 2,
  unicode = {true},
  breaklinks = {false},
  hyperindex = {true},
  colorlinks = {true},
  linktocpage = {true},
  plainpages = {false},
  linkcolor = {Blue},
  citecolor = {Blue},
  urlcolor = {Red},
  pdfstartview = {Fit},
  pdfpagemode = {UseOutlines},
  pdfview = {XYZ null null null}
}


\author{
  Laurent Gatto\thanks{\email{lg390@cam.ac.uk}}
}

\affil{
  Cambridge Center for Proteomics\\
  University of Cambridge
}


\begin{document}

\title{Machine learning techniques available in \Rpackage{pRoloc}}

\maketitle

% %% Abstract and keywords %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \vskip 0.3in minus 0.1in
% \hrule
% \begin{abstract}
% This document described the machine learning techniques that are made available in the \Rpackage{pRoloc}.
% \end{abstract}
% \textit{Keywords}: Bioinformatics, spatial/organelle proteomics, machine learning, visualisation
% \vskip 0.1in minus 0.05in
% \hrule
% \vskip 0.2in minus 0.1in
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newpage

\tableofcontents

<<env, include=FALSE, echo=FALSE, cache=FALSE>>=
library("knitr")
opts_chunk$set(fig.align = 'center', 
               fig.show = 'hold', 
               par = TRUE,
               prompt = TRUE,
               eval = TRUE,
               comment = NA)
options(replace.assign = TRUE, 
        width = 55)

suppressPackageStartupMessages(library("MSnbase"))
suppressWarnings(suppressPackageStartupMessages(library("pRoloc")))
## suppressPackageStartupMessages(library("pRolocdata"))
## suppressPackageStartupMessages(library("class"))
suppressPackageStartupMessages(library("xtable"))
@ 
%%$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage

TODO
\begin{itemize}
\item Define nomenclature
\end{itemize}

\section{Introduction}\label{sec:intro} 

For a general practical introduction to \Rpackage{pRoloc}, readers are referred to the tutorial, 
available using \Rfunction{vignette("pRoloc-tutorial", package = "pRoloc")}.
The following document provides a overview of the algorithms available in the package. 
The respective section describe unsupervised machine learning (USML), 
supervised machine learning (SML), and semi-supervised machine learning (SSML) 
as implemented in the novetly detection algorithm. 

\section{Unsupervised machine learning}\label{sec:usml}

Currently, unsupervised clustering facilities are available through the \Rfunction{plot2D} function 
and the \Rpackage{MLInterfaces} package \cite{MLInterfaces}. 
The former takes an \Robject{MSnSet} instance and represents the data on a scatter plot along the 
first two principal components. Arbitrary feature meta-data can be represented using different 
colours and point characters. The reader is referred to the manual page for more details 
and examples: \Rfunction{?plot2D}.

\Rpackage{pRoloc} also implements a \Rfunction{MLean} method for \Robject{MSnSet} instances, allowing to use the relevant infrastructure with the organelle proteomics framework. Although provides a common interface to unsupervised and numerous supervised algorithms, we refer to the \Rpackage{pRoloc} tutorial for its usage to several clustering algorithms.

\paragraph{Note} Current developments in terms of clustering are described on the \textit{Clustering infrastructure} wiki 
page\footnote{\url{https://github.com/lgatto/pRoloc/wiki/Clustering-infrastructure}} and will be incorporated in future version of the package.

\section{Supervised machine learning}\label{sec:sml}

Provide here a general description: training and testing, parameter optimisation.

\subsection{Algorithms used}\label{sec:algo}

\paragraph{k-nearest neighbour} Function \Rfunction{knn} from package \Rpackage{class}. 
$k$-nearest neighbour classification for test set from training set. For each row of the 
test set, the $k$ nearest (in Euclidean distance) training set vectors are found, and the 
classification is decided by majority vote, with ties broken at random. If there are ties 
for the $k^{th}$ nearest vector, all candidates are included in the vote.

\paragraph{Partial least square DA} Function \Rfunction{plsda} from package \Rpackage{caret}. 
Partial least square discriminant analysis is used to fit standard PLS models for classification.

\paragraph{Support vector machine} 
A support vector machine constructs a hyperplane (or set of hyperplanes for multiple-class problem), 
which are then used for classification. The best separation is defined 
as the hyperplane that has the largest distance (the margin) to the nearest data points in any class, 
which also reduces the classifiation generalisation error. To assure liner separation of the 
classes, the data is transformed using a \textit{kernel function} into a high-dimensional space, 
permitting liner separation of the classes.

\Rpackage{pRoloc} makes use of the functions \Rfunction{svm} from package \Rpackage{e1071} 
and \Rfunction{ksvm} from \Rpackage{kernlab}. 

\paragraph{Articifial neural network} Function \Rfunction{nnet} from package \Rpackage{nnet}. 
Fits a single-hidden-layer neural network, possibly with skip-layer connections.

\paragraph{Naive Bayes} Function \Rfunction{naiveBayes} from package \Rpackage{e1071}. 
Naive Bayes classifier that computes the conditional a-posterior probabilities of a 
categorical class variable given independent predictor variables using the Bayes rule. 
Assumes independence of the predictor variables, and Gaussian distribution 
(given the target class) of metric predictors.

\paragraph{Random Forest} Function \Rfunction{randomRandom} from package \Rpackage{randomForest}.

\paragraph{Chi-square} Assingnment based on squared differences between a labelled marker and a 
new feature to be classified. Canonical protein correlation profile method (PCP) uses sqaured 
differences between a labelled marker and new features. In \citep{Andersen2003}, $\chi^2$ is 
defined as \emph{the [summed] squared deviation of the normalized profile [from the marker] for 
  all peptides divided by the number of data points}, 
i.e. $\chi^{2} = \frac{\sum_{i=1}^{n} (x_i - m_i)^{2}}{n}$, whereas \citep{Wiese2007} divide 
by the value the squared value by the value of the reference feature in each fraction, 
i.e. $\chi^{2} = \sum_{i=1}^{n}\frac{(x_i - m_i)^{2}}{m_i}$, where $x_i$ is normalised intensity 
of feature $x$ in fraction $i$,  $m_i$ is the normalised intensity of marker $m$ in fraction 
$i$ and $n$ is the number of fractions available. We will use the former definition.

\paragraph{PerTurbo}
From \cite{perturbo}: 
PerTurbo, [is] an original, non-parametric and efficient classification method [...]. 
In our framework, the manifold of each class is characterized by its Laplace-Beltrami operator, 
which is evaluated with classical methods involving the graph Laplacian. 
The classification criterion is established thanks to a measure of the magnitude of the spectrum perturbation of this operator. 
The first experiments show good performances against classical algorithms of the state-of-the-art.
Moreover, from this measure is derived an efficient policy to design sampling queries in a context of active learning. 
Performances collected over toy examples and real world datasets assess the qualities of this strategy.

<<getmlfunction, echo=FALSE>>=
## Add chi^2.
tab <- data.frame('parameter optimisation' = grep("Optimisation", ls("package:pRoloc"), value = TRUE), 
                  'classification' = grep("Classification", ls("package:pRoloc"), value = TRUE))
tab$algorithm <- c("nearest neighbour", "support vector machine", "naive bayes",
                   "neural networks", "PerTurbo", "partial least square", 
                   "random forest", "support vector machine")
tab$package <- c("class", "kernlab", "e1071",
                 "nnet", "pRoloc", "caret",
                 "randomForest", "e1071")
colnames(tab)[1] <- c("parameter optimisation")
@ 

<<comptab, results='asis', echo=FALSE>>=
xt <- xtable(tab, label = "tab:algo",
             caption = "Supervised ML algorithm available in \\Rpackage{pRoloc}.")
print(xt, include.rownames = FALSE, size = "small")
@ 

\subsection{Default analysis scheme}

\minisec{Parameter optimisation}

The first step of our analysis scheme is a parameter optimisation. Algorithmic performance is estimated using 5-fold (default, see \Robject{xval} function parameter) stratified cross-validation (creating 5 test/train splits), which features an additional cross-validation on each training partition in order to optimise free parameters via a grid search. This process is repeated 10 times below (default is 100, see parameter \Robject{times}) and best and averaged accuracies are computed and stored in an instance of class \Robject{GenRegRes}. The object can be inspected, plotted and best parameter pairs can be extracted.

<<svmParamOptim, cache = TRUE, eval = FALSE>>=
params <- svmOptimisation(tan2009r1, times = 10, xval = 5, verbose = FALSE)
params
@ 

\minisec{Classification}

<<svmRes, warning=FALSE, tidy = FALSE, eval = FALSE>>=
svmres <- svmClassification(tan2009r1, params)
@ 

\subsection{Customising model parameters}

Below we illustrate how to weight different classes according to the 
number of labelled instances, were large sets are down weighted. 
This strategy can help with imbalanced designs.

<<weigths, eval=FALSE>>=
w <- table(fData(dunkley2006)$markers)
w <- 1/w[-5]
wpar <- svmOptimisation(dunkley2006, class.weights = w)
wres <- svmClassification(dunkley2006, pw, class.weights = w)
@ 

\section{Semi-supervised machine learning and noverly detection}\label{sec:ssml}

\singlespacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\clearpage

\section*{Session information}\label{sec:sessionInfo} 

All software and respective versions used to produce this document are listed below.

<<sessioninfo, results='asis', echo=FALSE>>=
toLatex(sessionInfo())
@

\bibliographystyle{plainnat}
\bibliography{pRoloc}

\end{document}

